# train_incremental.py
"""
å¢é‡å­¸ç¿’ï¼ˆé¤ŠAIï¼‰è¨“ç·´è…³æœ¬ï¼šæ¯æ¬¡æœ‰æ–°è³‡æ–™æ™‚ï¼Œè¼‰å…¥èˆŠæ¨¡å‹ï¼Œæ··åˆæ–°èˆŠè³‡æ–™è¨“ç·´ï¼Œé˜²æ­¢éºå¿˜
"""
import torch
from torch.utils.data import DataLoader, TensorDataset, random_split
import numpy as np
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import os
import random

from models import FallLSTM
import config


def load_data_for_incremental(old_data_path, new_data_path, replay_ratio=0.5):
    """
    è¼‰å…¥èˆŠè³‡æ–™èˆ‡æ–°è³‡æ–™ï¼Œä¸¦æ ¹æ“š replay_ratio æ··åˆ
    """
    X_old = np.load(os.path.join(old_data_path, "X.npy"), allow_pickle=True)
    y_old = np.load(os.path.join(old_data_path, "y.npy"), allow_pickle=True)
    X_new = np.load(os.path.join(new_data_path, "X_new.npy"), allow_pickle=True)
    y_new = np.load(os.path.join(new_data_path, "y_new.npy"), allow_pickle=True)

    # ç¶“é©—å›æ”¾ï¼šéš¨æ©ŸæŠ½å–éƒ¨åˆ†èˆŠè³‡æ–™
    n_replay = int(len(X_new) * replay_ratio)
    idxs = np.random.choice(len(X_old), min(n_replay, len(X_old)), replace=False)
    X_replay = X_old[idxs]
    y_replay = y_old[idxs]

    # åˆä½µæ–°èˆŠè³‡æ–™
    X_total = np.concatenate([X_new, X_replay], axis=0)
    y_total = np.concatenate([y_new, y_replay], axis=0)
    return X_total, y_total


def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ ä½¿ç”¨è£ç½®: {device}")

    # 1. è¼‰å…¥èˆŠæ¨¡å‹
    model = FallLSTM(input_size=config.INPUT_SIZE).to(device)
    if os.path.exists(config.MODEL_PATH):
        model.load_state_dict(torch.load(config.MODEL_PATH, map_location=device))
        print("ğŸ”„ è¼‰å…¥å·²å­˜åœ¨çš„æ¨¡å‹ï¼Œé€²è¡Œå¢é‡å­¸ç¿’")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°èˆŠæ¨¡å‹ï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´")

    # 2. è¼‰å…¥æ–°è³‡æ–™èˆ‡ replay èˆŠè³‡æ–™
    # æ–°è³‡æ–™è«‹å…ˆç¶“éè³‡æ–™å‰è™•ç†ï¼Œå­˜æˆ X_new.npy, y_new.npy
    X, y = load_data_for_incremental(config.DATA_DIR, config.DATA_DIR, replay_ratio=0.5)
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)
    dataset = TensorDataset(X_tensor, y_tensor)
    train_loader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=True)

    # 3. è¨“ç·´åƒæ•¸
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.0001)  # å­¸ç¿’ç‡è¼ƒä½ï¼Œé¿å…æ–°è³‡æ–™æ´—æ‰èˆŠçŸ¥è­˜
    writer = SummaryWriter(log_dir=config.LOG_DIR)

    # 4. å¢é‡è¨“ç·´
    EPOCHS = 20  # å¢é‡è¨“ç·´å¯è¨­è¼ƒå°‘ epoch
    for epoch in range(EPOCHS):
        print(f"\nâ³ å¢é‡è¨“ç·´ç¬¬ {epoch + 1}/{EPOCHS} æ¬¡...")
        model.train()
        train_loss = 0
        train_loop = tqdm(train_loader, desc=f"å¢é‡è¨“ç·´ Epoch {epoch + 1}", leave=False)
        for inputs, labels in train_loop:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            train_loop.set_postfix(loss=loss.item())
        train_loss /= len(train_loader)
        writer.add_scalar("Loss/Incremental_Train", train_loss, epoch + 1)
        print(f"ğŸ“Š å¢é‡è¨“ç·´ Epoch {epoch + 1} | Train Loss: {train_loss:.4f}")

    # 5. å„²å­˜æ–°æ¨¡å‹
    torch.save(model.state_dict(), config.MODEL_PATH)
    print("âœ… å¢é‡å­¸ç¿’å®Œæˆï¼Œæ¨¡å‹å·²æ›´æ–°ã€‚")
    writer.close()


if __name__ == "__main__":
    main()
